{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/training-data-for-music-prediction/training_data.csv\")\ntest = pd.read_csv(\"/kaggle/input/test-data-for-music-prediction/Test_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newcolumn1=df['month']\nnewcolumn2=df['date']\ndf.drop(\"month\",axis=1,inplace=True)\ndf.drop(\"date\",axis=1,inplace=True)\ndf['month']=newcolumn1\ndf['date']=newcolumn2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nid_test = test['id']\ndf.drop(\"id\",axis=1,inplace=True)\ntest.drop(\"id\",axis=1,inplace=True)\nle = LabelEncoder()\nmapper = {\"very low\": 1,\"low\": 2, \"average\": 3, \"high\": 4, \"very high\": 5}\ndf['popularity'] = df['popularity'].map(mapper)\ndf.head()\ndf['explicit'] = le.fit_transform(df['explicit'])\ntest['explicit'] = le.transform(test['explicit'])\ndf['mode'] = le.fit_transform(df['mode'])\ntest['mode'] = le.transform(test['mode'])\ndf.drop(\"release_date\",axis=1,inplace=True)\ntest.drop('release_date',axis=1,inplace=True)\ndf['days'] = (df['year']-1920)*365 + df['date']*12 + df['month']\ntest['days'] = (test['year']-1920)*365 + test['date']*12 + test['month']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['year_new']=pd.cut(x=df['year'], bins=[1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020,2030],labels=['20s','30s','40s','50s','60s','70s','80s','90s','00s','10s','2020s'])\ndf['year_class']=df['year_new'].astype(\"category\").cat.codes\ndf.drop(\"year_new\",axis=1,inplace=True)\ntest['year_new']=pd.cut(x=test['year'], bins=[1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020,2030],labels=['20s','30s','40s','50s','60s','70s','80s','90s','00s','10s','2020s'])\ntest['year_class']=test['year_new'].astype(\"category\").cat.codes\ntest.drop(\"year_new\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"energy\",axis=1,inplace=True)\ntest.drop(\"energy\",axis=1,inplace=True)\ncriteria= [df['acousticness'].between(0, 0.05),\n           df['acousticness'].between(0.050000000001, 0.9),\n           df['acousticness'].between(0.900000000001, 1)]\nvalues = [1, 2, 3]\ndf['acousticness_criteria'] = np.select(criteria, values, 0)\ncriteria= [test['acousticness'].between(0, 0.05),\n           test['acousticness'].between(0.050000000001, 0.9),\n           test['acousticness'].between(0.900000000001, 1)]\nvalues = [1, 2, 3]\ntest['acousticness_criteria'] = np.select(criteria, values, 0)\ncriteria= [df['duration-min'].between(0, 4),\n           df['duration-min'].between(4.0000000001, 10),\n           df['duration-min'].between(10.00000000001, 75)]\nvalues = [1, 2, 3]\ndf['duration_criteria'] = np.select(criteria, values, 0)\ncriteria= [test['duration-min'].between(0, 4),\n           test['duration-min'].between(4.0000000001, 10),\n           test['duration-min'].between(10.00000000001, 75)]\nvalues = [1, 2, 3]\ntest['duration_criteria'] = np.select(criteria, values, 0)\ncriteria= [df['instrumentalness'].between(0, 0.000115),\n           df['instrumentalness'].between(0.00011500001, 0.055650),\n           df['instrumentalness'].between(0.0556500001, 1)]\nvalues = [1, 2, 3]\ndf['instrumentalness_criteria'] = np.select(criteria, values, 0)\ncriteria= [test['instrumentalness'].between(0, 0.000115),\n           test['instrumentalness'].between(0.00011500001, 0.055650),\n           test['instrumentalness'].between(0.0556500001, 1)]\nvalues = [1, 2, 3]\ntest['instrumentalness_criteria'] = np.select(criteria, values, 0)\ncriteria= [df['liveness'].between(0, 0.1),\n           df['liveness'].between(0.100000001, 0.4),\n           df['liveness'].between(0.40000000001, 1)]\nvalues = [1, 2, 3]\ndf['liveness_criteria'] = np.select(criteria, values, 0)\ncriteria= [test['liveness'].between(0, 0.1),\n           test['liveness'].between(0.100000001, 0.4),\n           test['liveness'].between(0.40000000001, 1)]\nvalues = [1, 2, 3]\ntest['liveness_criteria'] = np.select(criteria, values, 0)\ndf['month_curse']=(df['date']==1)|(df['date']==12)\ntest['month_curse']=(test['date']==1)|(test['date']==12)\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf['month_curse']=le.fit_transform(df['month_curse'])\ntest['month_curse']=le.transform(test['month_curse'])\nle=LabelEncoder()\ndf['date_curse']=(df['month']==1)|(df['month']==31)\ndf['date_curse']=le.fit_transform(df['date_curse'])\ntest['date_curse']=(test['month']==1)|(test['month']==31)\ntest['date_curse']=le.transform(test['date_curse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def revmax(pred,y_test):\n  leftout=10000\n  revenue=0\n  for i,j in zip(pred,y_test):\n    if i>=j:\n      if leftout>=i:\n          leftout=leftout-i\n          revenue=revenue+ 2*j\n  print(leftout)\n  print(revenue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['select_speech']=df['speechiness']<0.55\ndf['select_speech']=le.fit_transform(df['select_speech'])\ntest['select_speech']=test['speechiness']<0.55\ntest['select_speech']=le.fit_transform(test['select_speech'])\ncriteria= [df['tempo'].between(0.000, 20),\n           df['tempo'].between(20.000001, 40),\n           df['tempo'].between(40.000001,60),\n           df['tempo'].between(60.000001,66),\n           df['tempo'].between(66.000001,76),\n           df['tempo'].between(76.000001,108),\n           df['tempo'].between(108.000001,120),\n           df['tempo'].between(120.000001,168),\n           df['tempo'].between(168.000001,200),\n           df['tempo'].between(200.000001,220)]\nvalues = [1, 2, 3,4,5,6,7,8,9,10]\ndf['temp_category'] = np.select(criteria, values, 0)\ncriteria= [test['tempo'].between(0.000, 20),\n           test['tempo'].between(20.000001, 40),\n           test['tempo'].between(40.000001,60),\n           test['tempo'].between(60.000001,66),\n           test['tempo'].between(66.000001,76),\n           test['tempo'].between(76.000001,108),\n           test['tempo'].between(108.000001,120),\n           test['tempo'].between(120.000001,168),\n           test['tempo'].between(168.000001,200),\n           test['tempo'].between(200.000001,220)]\nvalues = [1, 2, 3,4,5,6,7,8,9,10]\ntest['temp_category'] = np.select(criteria, values, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,pd.get_dummies(df['temp_category'], prefix='temp_category')],axis=1)\ndf.drop('temp_category',axis=1,inplace=True)\ntest = pd.concat([test,pd.get_dummies(test['temp_category'], prefix='temp_category')],axis=1)\ntest.drop('temp_category',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,pd.get_dummies(df['liveness_criteria'], prefix='liveness_criteria')],axis=1)\ndf.drop('liveness_criteria',axis=1,inplace=True)\ndf = pd.concat([df,pd.get_dummies(df['instrumentalness_criteria'], prefix='instrumentalness_criteria')],axis=1)\ndf.drop('instrumentalness_criteria',axis=1,inplace=True)\ndf = pd.concat([df,pd.get_dummies(df['acousticness_criteria'], prefix='acousticness_criteria')],axis=1)\ndf.drop('acousticness_criteria',axis=1,inplace=True)\ntest = pd.concat([test,pd.get_dummies(test['liveness_criteria'], prefix='liveness_criteria')],axis=1)\ntest.drop('liveness_criteria',axis=1,inplace=True)\ntest = pd.concat([test,pd.get_dummies(test['instrumentalness_criteria'], prefix='instrumentalness_criteria')],axis=1)\ntest.drop('instrumentalness_criteria',axis=1,inplace=True)\ntest = pd.concat([test,pd.get_dummies(test['acousticness_criteria'], prefix='acousticness_criteria')],axis=1)\ntest.drop('acousticness_criteria',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['key']=(test['key']-df['key'].min())/(df['key'].max()-df['key'].min())\ntest['loudness']=(test['loudness']+43.736)/(1.006+43.736)\ntest['tempo']=(test['tempo']-df['tempo'].min())/(df['tempo'].max()-df['tempo'].min())\ntest['duration-min']=(test['duration-min']-df['duration-min'].min())/(df['duration-min'].max()-df['duration-min'].min())\ntest['days']=(test['days']-df['days'].min())/(df['days'].max()-df['days'].min())\nscaled_features = ['key','loudness','tempo','duration-min','days']\nfor s in scaled_features:\n    df[s] = (df[s]-df[s].min())/(df[s].max()-df[s].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['popularity']\ndf.drop(\"popularity\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=10, random_state=1)\nnew = df._get_numeric_data().dropna(axis=1)\nkm.fit(df)\npredict=km.predict(df)\ndf['kmeans']= predict\npredictnew = km.predict(test)\ntest['kmeans']=predictnew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversampling = SMOTE()\nundersampler = RandomUnderSampler(random_state = 1)\ndfover,yover = oversampling.fit_resample(df,y)\nprint(dfover.shape)\ndflow,ylow = undersampler.fit_resample(df,y)\nprint(dflow.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline1 = RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.2, min_samples_leaf=5, min_samples_split=4, n_estimators=100)\nexported_pipeline1.fit(dfover, yover)\nprobs1 = exported_pipeline1.predict_proba(test)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline1low = RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.2, min_samples_leaf=5, min_samples_split=4, n_estimators=100)\nexported_pipeline1low.fit(dflow, ylow)\nprobs1low = exported_pipeline1.predict_proba(test)\n\nexported_pipeline2 = XGBClassifier()\nexported_pipeline2.fit(dfover, yover)\nprobs2 = exported_pipeline2.predict_proba(test)\n\nexported_pipeline2low = XGBClassifier()\nexported_pipeline2low.fit(dflow, ylow)\nprobs2low = exported_pipeline2low.predict_proba(test)\n\nexported_pipeline3= LGBMClassifier()\nexported_pipeline3.fit(dfover, yover)\nprobs3 = exported_pipeline3.predict_proba(test)\n\nexported_pipeline3low = LGBMClassifier()\nexported_pipeline3low.fit(dflow, ylow)\nprobs3low = exported_pipeline3low.predict_proba(test)\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nexported_pipeline4= GradientBoostingClassifier()\nexported_pipeline4.fit(dfover, yover)\nprobs4 = exported_pipeline4.predict_proba(test)\n\nexported_pipeline4low = GradientBoostingClassifier()\nexported_pipeline4low.fit(dflow, ylow)\nprobs4low = exported_pipeline4low.predict_proba(test)\n\nexported_pipeline5= KNeighborsClassifier(n_neighbors=34)\nexported_pipeline5.fit(dfover, yover)\nprobs5 = exported_pipeline5.predict_proba(test)\n\nexported_pipeline5low = KNeighborsClassifier(n_neighbors=31)\nexported_pipeline5low.fit(dflow, ylow)\nprobs5low = exported_pipeline5low.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs1low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs2low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs3low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs4low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs5low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalactualprob=(probs1low+probs1+probs5low+probs5+probs2+probs2low+probs3+probs3low+probs4+probs4low)/10.00\nynew_pred = np.argmax(totalactualprob,axis=1)\nynew_pred = ynew_pred+1\nprint(ynew_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(totalactualprob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ynew_pred.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({ 'id' : id_test, 'popularity1': ynew_pred})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapper = {1:\"very low\",2:\"low\",3:\"average\",4:\"high\",5:\"very high\"}\noutput['popularity'] = output['popularity1'].map(mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapper = {1:\"very low\",2:\"low\",3:\"average\",4:\"high\",5:\"very high\"}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.drop(\"popularity1\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('TEAM36_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(output['popularity'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_list = data[:, col]\nset(column_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['popularity']=y\ndfnew =df\ny=dfnew['popularity']\ndfnew.drop(\"popularity\",axis=1,inplace=True)\nX_train=dfnew[4000:]\ny_train=y[4000:]\nX_test=dfnew[0:4000]\ny_test=y[0:4000]\nX_trainlow=dfnew[4000:]\ny_trainlow=y[4000:]\nX_testlow=dfnew[0:4000]\ny_testlow=y[0:4000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfover.drop(\"popularity\",axis=1,inplace=True)\nX_train,X_test,y_train,y_test=train_test_split(dfover,y,test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train,X_test,y_train,y_test=train_test_split(dfover,y,test_size=0.33)\n\nX_trainlow,X_testlow,y_trainlow,y_testlow=train_test_split(dflow,y,test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\noversampling = SMOTE()\nundersampler = RandomUnderSampler(random_state = 1)\nX_train,y_train = oversampling.fit_resample(X_train,y_train)\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\noversampling = SMOTE()\nundersampler = RandomUnderSampler(random_state = 1)\nX_trainlow,y_trainlow = undersampler.fit_resample(X_trainlow,y_trainlow)\nprint(X_trainlow.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline1 = RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.2, min_samples_leaf=5, min_samples_split=4, n_estimators=100)\nexported_pipeline1.fit(X_train, y_train)\nresults = exported_pipeline1.predict(X_test)\nprint(accuracy_score(results,y_test))\nprint(confusion_matrix(results,y_test))\nprint(revmax(results,y_test))\nprobs1 = exported_pipeline1.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exported_pipeline1.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbcimp=exported_pipeline1.feature_importances_\ngbccolumn = dfnew.columns\nfig, ax = plt.subplots(figsize = (30,20))\nsns.barplot(gbcimp,gbccolumn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nexported_pipeline1 = RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.2, min_samples_leaf=5, min_samples_split=4, n_estimators=100)\nexported_pipeline1.fit(X_trainlow, y_trainlow)\nresults = exported_pipeline1.predict(X_testlow)\nprint(accuracy_score(results,y_testlow))\nprint(confusion_matrix(results,y_testlow))\nprint(revmax(results,y_testlow))\nprobs1low = exported_pipeline1.predict_proba(X_testlow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nexported_pipeline2 = XGBClassifier()\nexported_pipeline2.fit(X_train, y_train)\nresults = exported_pipeline2.predict(X_test)\nprint(accuracy_score(results,y_test))\nprint(confusion_matrix(results,y_test))\nprint(revmax(results,y_test))\nprobs2 = exported_pipeline2.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline2low = XGBClassifier()\nexported_pipeline2low.fit(X_trainlow, y_trainlow)\nresults = exported_pipeline2low.predict(X_testlow)\nprint(accuracy_score(results,y_testlow))\nprint(confusion_matrix(results,y_testlow))\nprint(revmax(results,y_testlow))\nprobs2low = exported_pipeline2low.predict_proba(X_testlow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nexported_pipeline3= LGBMClassifier()\nexported_pipeline3.fit(X_train, y_train)\nresults = exported_pipeline3.predict(X_test)\nprint(accuracy_score(results,y_test))\nprint(confusion_matrix(results,y_test))\nprint(revmax(results,y_test))\nprobs3 = exported_pipeline3.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline3low = LGBMClassifier()\nexported_pipeline3low.fit(X_trainlow, y_trainlow)\nresults = exported_pipeline3low.predict(X_testlow)\nprint(accuracy_score(results,y_testlow))\nprint(confusion_matrix(results,y_testlow))\nprint(revmax(results,y_testlow))\nprobs3low = exported_pipeline3low.predict_proba(X_testlow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nexported_pipeline4= GradientBoostingClassifier()\nexported_pipeline4.fit(X_train, y_train)\nresults = exported_pipeline4.predict(X_test)\nprint(accuracy_score(results,y_test))\nprint(confusion_matrix(results,y_test))\nprint(revmax(results,y_test))\nprobs4 = exported_pipeline4.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline4low = GradientBoostingClassifier()\nexported_pipeline4low.fit(X_trainlow, y_trainlow)\nresults = exported_pipeline4low.predict(X_testlow)\nprint(accuracy_score(results,y_testlow))\nprint(confusion_matrix(results,y_testlow))\nprint(revmax(results,y_testlow))\nprobs4low = exported_pipeline4low.predict_proba(X_testlow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nexported_pipeline5= KNeighborsClassifier(n_neighbors=34)\nexported_pipeline5.fit(X_train, y_train)\nresults = exported_pipeline5.predict(X_test)\nprint(accuracy_score(results,y_test))\nprint(confusion_matrix(results,y_test))\nprint(revmax(results,y_test))\nprobs5 = exported_pipeline5.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n# Average CV score on the training set was: 0.6820680018630647\nexported_pipeline5low = KNeighborsClassifier(n_neighbors=31)\nexported_pipeline5low.fit(X_trainlow, y_trainlow)\nresults = exported_pipeline5low.predict(X_testlow)\nprint(accuracy_score(results,y_testlow))\nprint(confusion_matrix(results,y_testlow))\nprint(revmax(results,y_testlow))\nprobs5low = exported_pipeline5low.predict_proba(X_testlow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalprob=(probs1low+probs2low+probs3low+probs4low+probs5low)/5\nynew_pred = np.argmax(totalprob,axis=1)\nynew_pred=ynew_pred+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(ynew_pred,y_testlow))\nprint(confusion_matrix(ynew_pred,y_testlow))\nprint(revmax(ynew_pred,y_testlow))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalprob=(probs1+probs2+probs3+probs4+probs5)/5\nynew_pred = np.argmax(totalprob,axis=1)\nynew_pred=ynew_pred+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(ynew_pred,y_test))\nprint(confusion_matrix(ynew_pred,y_test))\nprint(revmax(ynew_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalactualprob=(probs1low+probs2low+probs3low+probs4low+probs5low+probs1+probs2+probs3+probs4+probs5)/10\nynew_pred = np.argmax(totalactualprob,axis=1)\nynew_pred=ynew_pred+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(ynew_pred,y_test))\nprint(confusion_matrix(ynew_pred,y_test))\nprint(revmax(ynew_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exported_pipeline1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfor i in range(0,10):\n    print(\"level:\",i)\n    X_train,X_test,y_train,y_test=train_test_split(df,y,test_size=0.33)\n    over = SMOTE()\n    undersampler = RandomUnderSampler(random_state = 1)\n    X_train, y_train = over.fit_resample(X_train, y_train)\n    exported_pipeline = LGBMClassifier()\n    exported_pipeline.fit(X_train, y_train)\n    results = exported_pipeline.predict(X_test)\n    print(accuracy_score(results,y_test))\n    print(confusion_matrix(results,y_test))\n    print(revmax(results,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbcimp=exported_pipeline1.feature_importances_\ngbccolumn = X_train.columns\nfig, ax = plt.subplots(figsize = (30,20))\nsns.barplot(gbcimp,gbccolumn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If the year is 2021, we are using a different model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,pd.get_dummies(df['temp_category'], prefix='temp_category')],axis=1)\ndf.drop('temp_category',axis=1,inplace=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}